{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install altair\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Oct  8 13:53:07 2020\n",
    "\n",
    "@author: ricks\n",
    "\"\"\"\n",
    "import pathlib\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def add_attacking_direction(eventsDF, tdatDF, playersDBDF, tMetaDF):\n",
    "\n",
    "    attacking_directions = dict()\n",
    "\n",
    "    home_gk = playersDBDF.loc[(playersDBDF[\"position\"] == \"Goalkeeper\")].loc[0][\n",
    "        \"jersey_no\"\n",
    "    ]\n",
    "\n",
    "    gk_starting_position = tdatDF.loc[\n",
    "        (tdatDF[\"frameID\"] == tMetaDF[\"period1_start\"])\n",
    "        & (tdatDF[\"team\"] == 1)\n",
    "        & (tdatDF[\"jersey_no\"] == int(home_gk))\n",
    "    ][\"x\"]\n",
    "\n",
    "    if int(gk_starting_position) > 0:\n",
    "\n",
    "        attacking_directions[\"team1_period1\"] = 1\n",
    "        attacking_directions[\"team0_period1\"] = -1\n",
    "        attacking_directions[\"team1_period2\"] = -1\n",
    "        attacking_directions[\"team0_period2\"] = 1\n",
    "\n",
    "    else:\n",
    "\n",
    "        attacking_directions[\"team1_period1\"] = -1\n",
    "        attacking_directions[\"team0_period1\"] = 1\n",
    "        attacking_directions[\"team1_period2\"] = 1\n",
    "        attacking_directions[\"team0_period2\"] = -1\n",
    "\n",
    "    if tMetaDF[\"period3_end\"] != 0:\n",
    "\n",
    "        home_gk = playersDBDF.loc[(playersDBDF[\"position\"] == \"Goalkeeper\")].loc[0][\n",
    "            \"jersey_no\"\n",
    "        ]\n",
    "\n",
    "        gk_starting_position = tdatDF.loc[\n",
    "            (tdatDF[\"frameID\"] == tMetaDF[\"period3_start\"])\n",
    "            & (tdatDF[\"team\"] == 1)\n",
    "            & (tdatDF[\"jersey_no\"] == int(home_gk))\n",
    "        ][\"x\"]\n",
    "\n",
    "        if int(gk_starting_position) > 0:\n",
    "\n",
    "            attacking_directions[\"team1_period3\"] = 1\n",
    "            attacking_directions[\"team0_period3\"] = -1\n",
    "            attacking_directions[\"team1_period4\"] = -1\n",
    "            attacking_directions[\"team0_period4\"] = 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            attacking_directions[\"team1_period3\"] = -1\n",
    "            attacking_directions[\"team0_period3\"] = 1\n",
    "            attacking_directions[\"team1_period4\"] = 1\n",
    "            attacking_directions[\"team0_period4\"] = -1\n",
    "\n",
    "    else:\n",
    "\n",
    "        attacking_directions[\"team1_period3\"] = 0\n",
    "        attacking_directions[\"team0_period3\"] = 0\n",
    "        attacking_directions[\"team1_period4\"] = 0\n",
    "        attacking_directions[\"team0_period4\"] = 0\n",
    "\n",
    "    team_reference = playersDBDF[[\"team_id\", \"team\"]].drop_duplicates()\n",
    "    team_reference = team_reference.reset_index(drop=True)\n",
    "\n",
    "    eventsDF = eventsDF.merge(\n",
    "        team_reference, left_on=\"team_id\", right_on=\"team_id\", how=\"outer\"\n",
    "    )\n",
    "\n",
    "    eventsDF[\"attacking_direction\"] = 0\n",
    "\n",
    "    for i in range(0, len(eventsDF)):\n",
    "\n",
    "        ball_to_assess = eventsDF.loc[i]\n",
    "\n",
    "        if ball_to_assess[\"period_id\"] == 1:\n",
    "\n",
    "            if ball_to_assess[\"team\"] == 1:\n",
    "                eventsDF.at[i, \"attacking_direction\"] = attacking_directions[\n",
    "                    \"team1_period1\"\n",
    "                ]\n",
    "\n",
    "            elif ball_to_assess[\"team\"] == 0:\n",
    "                eventsDF.at[i, \"attacking_direction\"] = attacking_directions[\n",
    "                    \"team0_period1\"\n",
    "                ]\n",
    "\n",
    "        if ball_to_assess[\"period_id\"] == 2:\n",
    "\n",
    "            if ball_to_assess[\"team\"] == 1:\n",
    "                eventsDF.at[i, \"attacking_direction\"] = attacking_directions[\n",
    "                    \"team1_period2\"\n",
    "                ]\n",
    "\n",
    "            elif ball_to_assess[\"team\"] == 0:\n",
    "                eventsDF.at[i, \"attacking_direction\"] = attacking_directions[\n",
    "                    \"team0_period2\"\n",
    "                ]\n",
    "\n",
    "        if ball_to_assess[\"period_id\"] == 3:\n",
    "\n",
    "            if ball_to_assess[\"team\"] == 1:\n",
    "                eventsDF.at[i, \"attacking_direction\"] = attacking_directions[\n",
    "                    \"team1_period3\"\n",
    "                ]\n",
    "\n",
    "            elif ball_to_assess[\"team\"] == 0:\n",
    "                eventsDF.at[i, \"attacking_direction\"] = attacking_directions[\n",
    "                    \"team0_period3\"\n",
    "                ]\n",
    "\n",
    "        if ball_to_assess[\"period_id\"] == 4:\n",
    "\n",
    "            if ball_to_assess[\"team\"] == 1:\n",
    "                eventsDF.at[i, \"attacking_direction\"] = attacking_directions[\n",
    "                    \"team1_period4\"\n",
    "                ]\n",
    "\n",
    "            elif ball_to_assess[\"team\"] == 0:\n",
    "                eventsDF.at[i, \"attacking_direction\"] = attacking_directions[\n",
    "                    \"team0_period4\"\n",
    "                ]\n",
    "\n",
    "    return eventsDF\n",
    "\n",
    "\n",
    "def create_playerDB(f7_filename):\n",
    "\n",
    "    \"\"\"\n",
    "    Function that parses an f7 file to a workable pandas dataframe.\n",
    "\n",
    "    The input is an xml that contains valuable playerinformation.\n",
    "\n",
    "    Different steps are taken to get to the final database.\n",
    "    These steps are based on the strange and weird structure of the xml file which was hard to work with.\n",
    "\n",
    "    1. Create a dataframe of all strating players. This contains:\n",
    "        - Formation place\n",
    "        - playerID\n",
    "        - Position\n",
    "        - shirt number\n",
    "        - status (Starter or substitute)\n",
    "    2. create a dataframe of all player info. This contains:\n",
    "        - playerID\n",
    "        - First_name\n",
    "        - last_name\n",
    "        - full_name\n",
    "    3. merge both dataframes\n",
    "    4. Add playing time and work around substitutions. This contains:\n",
    "        - full time in minutes\n",
    "        - minutes seperately\n",
    "        - seconds seperately\n",
    "    5. Add teamID for each player (THIS WAS TRICKY, NEED TO CHECK FOR MULTIPLE MATCHES)\n",
    "\n",
    "    output is dataframe containing the above\n",
    "    \"\"\"\n",
    "\n",
    "    tree = ET.parse(f7_filename)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # match_id = int(root.find(\"SoccerDocument\").get(\"uID\")[1:])\n",
    "\n",
    "    gameinfo = root.findall(\"SoccerDocument\")[0]\n",
    "    # gameinfo = gameinfo_1[0]\n",
    "\n",
    "    formation_place = []\n",
    "    player_id = []\n",
    "    position = []\n",
    "    jersey_no = []\n",
    "    status = []\n",
    "\n",
    "    for neighbor in gameinfo.iter(\"MatchPlayer\"):\n",
    "        formation_place.append(neighbor.get(\"Formation_Place\"))\n",
    "        player_id.append(neighbor.get(\"PlayerRef\")[1:])\n",
    "        position.append(neighbor.get(\"Position\"))\n",
    "        jersey_no.append(neighbor.get(\"ShirtNumber\"))\n",
    "        status.append(neighbor.get(\"Status\"))\n",
    "\n",
    "    starting_players = pd.DataFrame(\n",
    "        {\n",
    "            \"formation_place\": formation_place,\n",
    "            \"player_id\": player_id,\n",
    "            \"position\": position,\n",
    "            \"jersey_no\": jersey_no,\n",
    "            \"status\": status,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    p_id = []\n",
    "    first_name = []\n",
    "    last_name = []\n",
    "    player_name = []\n",
    "\n",
    "    for neighbor in gameinfo.iter(\"Player\"):\n",
    "        p_id.append(neighbor.get(\"uID\")[1:])\n",
    "        first_name.append(neighbor.find(\"PersonName\").find(\"First\").text)\n",
    "        last_name.append(neighbor.find(\"PersonName\").find(\"Last\").text)\n",
    "        player_name.append(first_name[-1] + \" \" + last_name[-1])\n",
    "\n",
    "    bench_players = pd.DataFrame(\n",
    "        {\n",
    "            \"first_name\": first_name,\n",
    "            \"player_id\": p_id,\n",
    "            \"last_name\": last_name,\n",
    "            \"player_name\": player_name,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    players = starting_players.merge(bench_players, on=\"player_id\", how=\"inner\")\n",
    "\n",
    "    time = []\n",
    "    period_id = []\n",
    "    player_off = []\n",
    "    player_on = []\n",
    "\n",
    "    for neighbor in gameinfo.iter(\"Substitution\"):\n",
    "        time.append(int(neighbor.get(\"Min\")) + int(neighbor.get(\"Sec\")) / 60)\n",
    "        period_id.append(neighbor.get(\"Period\"))\n",
    "        player_off.append(neighbor.get(\"SubOff\")[1:])\n",
    "        if not neighbor.get(\"Retired\") == '1':\n",
    "            player_on.append(neighbor.get(\"SubOn\")[1:])\n",
    "        else:\n",
    "            player_on.append('None')\n",
    "    subs = pd.DataFrame(\n",
    "        {\n",
    "            \"time\": time,\n",
    "            \"period_id\": period_id,\n",
    "            \"player_off\": player_off,\n",
    "            \"player_on\": player_on,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    players[\"start_min\"] = 0\n",
    "    players[\"end_min\"] = 0\n",
    "\n",
    "    for neighbor in gameinfo.iter(\"Stat\"):\n",
    "        if neighbor.get(\"Type\") == \"match_time\":\n",
    "            match_length = int(neighbor.text)\n",
    "\n",
    "    players.loc[players[\"status\"] == \"Start\", \"end_min\"] = match_length\n",
    "\n",
    "    for index, content in subs.iterrows():\n",
    "        players.loc[players[\"player_id\"] == content[\"player_off\"], \"end_min\"] = content[\n",
    "            \"time\"\n",
    "        ]\n",
    "        players.loc[\n",
    "            players[\"player_id\"] == content[\"player_on\"], \"start_min\"\n",
    "        ] = content[\"time\"]\n",
    "        players.loc[\n",
    "            players[\"player_id\"] == content[\"player_on\"], \"end_min\"\n",
    "        ] = match_length\n",
    "\n",
    "    for neighbor in gameinfo.iter(\"Booking\"):\n",
    "        if neighbor.get(\"Card\") == \"Red\":\n",
    "            players.loc[\n",
    "                players[\"player_id\"] == neighbor.get(\"PlayerRef\")[1:], \"end_min\"\n",
    "            ] = (int(neighbor.get(\"Min\")) + int(neighbor.get(\"Sec\")) / 60)\n",
    "\n",
    "    players[\"mins_played\"] = players[\"end_min\"] - players[\"start_min\"]\n",
    "\n",
    "    # players[\"match_id\"] = match_id\n",
    "\n",
    "    home_away = []\n",
    "\n",
    "    for team in gameinfo.findall(\"Team\"):\n",
    "        home_away.append(team.get(\"uID\")[1:])\n",
    "\n",
    "    players = players[players.mins_played != 0]\n",
    "    players = players.reset_index(drop=True)\n",
    "\n",
    "    subs_index = players[(players.status == \"Sub\")].index\n",
    "    diff_subs_index = np.diff(subs_index)\n",
    "\n",
    "    for i in range(len(subs_index) - 1):\n",
    "        if subs_index[i + 1] - subs_index[i] > 1:\n",
    "            index_finder = subs_index[i] + 1\n",
    "            players.loc[:index_finder, \"team\"] = home_away[0]\n",
    "            players.loc[index_finder:, \"team\"] = home_away[1]\n",
    "\n",
    "    if np.max(diff_subs_index) == 1:\n",
    "        if np.min(subs_index) < 20:\n",
    "            index_finder = 11\n",
    "            players.loc[:index_finder, \"team\"] = home_away[0]\n",
    "            players.loc[index_finder:, \"team\"] = home_away[1]\n",
    "\n",
    "        else:\n",
    "            index_finder = 11\n",
    "            players.loc[:index_finder, \"team\"] = home_away[0]\n",
    "            players.loc[index_finder:, \"team\"] = home_away[1]\n",
    "\n",
    "    return players, home_away\n",
    "\n",
    "\n",
    "def parse_f24(file_name):\n",
    "\n",
    "    # parse the xml and convert to a tree and root\n",
    "    tree = ET.parse(file_name)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # get the main game info from the single 'Game' node\n",
    "    gameinfo = root.findall(\"Game\")\n",
    "    gameinfo = gameinfo[0]\n",
    "    game_id = gameinfo.get(\"id\")\n",
    "    home_team_id = gameinfo.get(\"home_team_id\")\n",
    "    home_team_name = gameinfo.get(\"home_team_name\")\n",
    "    away_team_id = gameinfo.get(\"away_team_id\")\n",
    "    away_team_name = gameinfo.get(\"away_team_name\")\n",
    "    competition_id = gameinfo.get(\"competition_id\")\n",
    "    competition_name = gameinfo.get(\"competition_name\")\n",
    "    season_id = gameinfo.get(\"season_id\")\n",
    "\n",
    "    Edata_columns = [\n",
    "        \"id\",\n",
    "        \"event_id\",\n",
    "        \"type_id\",\n",
    "        \"period_id\",\n",
    "        \"min\",\n",
    "        \"sec\",\n",
    "        \"outcome\",\n",
    "        \"player_id\",\n",
    "        \"team_id\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        \"sequence_id\",\n",
    "        \"possession_id\",\n",
    "    ]\n",
    "\n",
    "    Q_ids = []\n",
    "    Q_values = []\n",
    "    Edata = []\n",
    "\n",
    "    # loop through each ball node and grab the information\n",
    "    for i in root.iter(\"Event\"):\n",
    "\n",
    "        # get the info from the ball node main chunk\n",
    "        id_ = int(i.get(\"id\"))\n",
    "        event_id = i.get(\"event_id\")\n",
    "        type_id = i.get(\"type_id\")\n",
    "        period_id = int(i.get(\"period_id\"))\n",
    "        outcome = int(i.get(\"outcome\"))\n",
    "        min_ = int(i.get(\"min\"))\n",
    "        sec = int(i.get(\"sec\"))\n",
    "        player_id = i.get(\"player_id\")\n",
    "        team_id = i.get(\"team_id\")\n",
    "        x = i.get(\"x\")\n",
    "        y = i.get(\"y\")\n",
    "        possession_id = i.get(\"possession_id\")\n",
    "        sequence_id = i.get(\"sequence_id\")\n",
    "\n",
    "        Edata_values = [\n",
    "            id_,\n",
    "            event_id,\n",
    "            type_id,\n",
    "            period_id,\n",
    "            min_,\n",
    "            sec,\n",
    "            outcome,\n",
    "            player_id,\n",
    "            team_id,\n",
    "            x,\n",
    "            y,\n",
    "            sequence_id,\n",
    "            possession_id,\n",
    "        ]\n",
    "\n",
    "        # find all of the Q information for that file\n",
    "        Qs = i.findall(\"./Q\")\n",
    "\n",
    "        # create some empty lists to append the results to\n",
    "        qualifier_id = []\n",
    "        Q_value = []\n",
    "\n",
    "        # loop through all of the Qs and grab the info\n",
    "        for child in Qs:\n",
    "            qualifier_id.append(child.get(\"qualifier_id\"))\n",
    "            Q_value.append(child.get(\"value\", default=\"1\"))\n",
    "\n",
    "        Q_ids.append(qualifier_id)\n",
    "        Q_values.append(Q_value)\n",
    "        Edata.append(Edata_values)\n",
    "\n",
    "    # Stack all ball Data\n",
    "    df = pd.DataFrame(np.vstack(Edata), columns=Edata_columns)\n",
    "\n",
    "    unique_Q_ids = np.unique(np.concatenate(Q_ids))\n",
    "\n",
    "    # create an array for fast assignments\n",
    "    Qarray = np.zeros((df.shape[0], len(unique_Q_ids)))\n",
    "    Qarray = Qarray.astype(\"O\")\n",
    "    Qarray[:] = np.nan\n",
    "\n",
    "    # dict to relate Q_ids to array indices\n",
    "    keydict = dict(zip(unique_Q_ids, range(len(unique_Q_ids))))\n",
    "\n",
    "    # iter through all Q_ids, Q_values, assign values to appropriate indices\n",
    "    for idx, (i, v) in enumerate(zip(Q_ids, Q_values)):\n",
    "        Qarray[idx, [keydict.get(q) for q in Q_ids[idx]]] = Q_values[idx]\n",
    "\n",
    "    # df from array\n",
    "    Qdf = pd.DataFrame(Qarray, columns=unique_Q_ids, index=df.index)\n",
    "\n",
    "    # combine\n",
    "    game_df = pd.concat([df, Qdf], axis=1)\n",
    "\n",
    "    # assign game values\n",
    "    game_df[\"competition_id\"] = competition_id\n",
    "    game_df[\"game_id\"] = game_id\n",
    "    game_df[\"home_team_id\"] = home_team_id\n",
    "    game_df[\"home_team_name\"] = home_team_name\n",
    "    game_df[\"away_team_id\"] = away_team_id\n",
    "    game_df[\"away_team_name\"] = away_team_name\n",
    "    game_df[\"competition_id\"] = competition_id\n",
    "    game_df[\"competition_name\"] = competition_name\n",
    "    game_df[\"season_id\"] = season_id\n",
    "    game_df[\"competition_id\"] = competition_id\n",
    "\n",
    "    game_df[[\"id\", \"period_id\", \"min\", \"sec\", \"outcome\", \"140\", \"141\"]] = game_df[\n",
    "        [\"id\", \"period_id\", \"min\", \"sec\", \"outcome\", \"140\", \"141\"]\n",
    "    ].astype(\"float\")\n",
    "\n",
    "    game_df[\"x\"] = pd.to_numeric(game_df[\"x\"])\n",
    "    game_df[\"y\"] = pd.to_numeric(game_df[\"y\"])\n",
    "\n",
    "    game_df[['x', '140']] = game_df[['x', '140']] / 100 * 105\n",
    "    game_df[['y', '141']] = game_df[['y', '141']] / 100 * 68\n",
    "    for i in root.iter(\"Game\"):\n",
    "        play_date = i.get(\"game_date\").split('T')[0]   \n",
    "\n",
    "    # write to csv\n",
    "    return game_df, play_date\n",
    "\n",
    "\n",
    "def parse_tracab(\n",
    "    tracking_filename: Union[str, pathlib.Path],\n",
    "    game_metadata: pd.DataFrame,\n",
    "    home_away: List,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Parse a tracab.dat file and convert it to a workable pandas dataframe.\n",
    "\n",
    "    Tracking_filename: File containing tracking data of all players and the ball\n",
    "        File contains:\n",
    "            - FrameID = captured frame of datapoints\n",
    "            - team = 1: home and 0: away 10: ball\n",
    "            - target_id = set player to a nummeric value (make data anonymous). Value of 100 for ball.\n",
    "            - jersey_nu = players jersey number, 999 for ball\n",
    "            - x = position of player/ball along the length of the pitch in meters\n",
    "            - y = position of player/ball olong the width of the pitch in meters\n",
    "            - z = height of the ball in meters\n",
    "            - owning_team = team in possession of the ball A for away and H for Home\n",
    "            - ball_status = ball in or out of play: Alive = in play and Dead = ball out of play\n",
    "            - Ball_contact = value for the ball not sure what is here. B4: when ball is dead;\n",
    "              Whistle, SetHome, SetAway\n",
    "\n",
    "    Metadata_filename: File containing\n",
    "        File contains:\n",
    "            - Values corresponding to the starting frame of the first and second half\n",
    "            - Values corresponding to the ending frame of the first and second half\n",
    "            - pitch length and pitch width in meters\n",
    "\n",
    "    After parsing all the data based on the settings below the dataframe can be cleaned.\n",
    "    Set values for removing officials and tream_dead_time\n",
    "    removing officials = true -> remove officials; false = don't remove\n",
    "    trim_dead_time = True -> keep data when ball is out of play; false = delete data when ball is out of play\n",
    "\n",
    "    Output: Dataframe of the entire match of approximately 3.3 milion datapoints and 10 columns\n",
    "    \"\"\"\n",
    "\n",
    "    remove_officials = True\n",
    "    trim_dead_time = True\n",
    "\n",
    "    # First save all the dataframes/lines from the .dat file in a list\n",
    "    # This will take a lot of time initially\n",
    "\n",
    "    with open(tracking_filename) as fn:\n",
    "        file_content = fn.readlines()\n",
    "\n",
    "    content_raw = [x.strip() for x in file_content]\n",
    "\n",
    "    # Create empty lists to store the data per variable\n",
    "    frameID = []\n",
    "    team = []\n",
    "    target_id = []\n",
    "    jersey_no = []\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    speed = []\n",
    "    ball_owning_team = []\n",
    "    ball_status = []\n",
    "\n",
    "    for data_row in content_raw:\n",
    "\n",
    "        data_split = data_row.replace(\":\", \";\").split(\";\")\n",
    "        data_split = list(filter(None, data_split))\n",
    "\n",
    "        ball_data_split = data_split[-1].split(\",\")\n",
    "\n",
    "        frameID.append(int(data_split[0]))\n",
    "        team.append(\"10\")\n",
    "        target_id.append(\"100\")\n",
    "        jersey_no.append(\"999\")\n",
    "        x.append(float(ball_data_split[0]))\n",
    "        y.append(float(ball_data_split[1]))\n",
    "        z.append(float(ball_data_split[2]))\n",
    "        speed.append(float(ball_data_split[3]))\n",
    "        ball_owning_team.append(ball_data_split[4])\n",
    "        ball_status.append(ball_data_split[5])\n",
    "\n",
    "        for content in data_split[1:-1]:\n",
    "            split_content = content.split(\",\")\n",
    "            frameID.append(int(data_split[0]))\n",
    "            team.append(split_content[0])\n",
    "            target_id.append(split_content[1])\n",
    "            jersey_no.append(split_content[2])\n",
    "            x.append(float(split_content[3]))\n",
    "            y.append(float(split_content[4]))\n",
    "            z.append(float(0.0))\n",
    "            speed.append(float(split_content[5]))\n",
    "            ball_owning_team.append(ball_data_split[4])\n",
    "            ball_status.append(ball_data_split[5])\n",
    "\n",
    "    all_tracking_data = pd.DataFrame(\n",
    "        {\n",
    "            \"frameID\": frameID,\n",
    "            \"team\": team,\n",
    "            \"target_id\": target_id,\n",
    "            \"jersey_no\": jersey_no,\n",
    "            \"x\": x,\n",
    "            \"y\": y,\n",
    "            \"z\": z,\n",
    "            \"speed\": speed,\n",
    "            \"ball_owning_team\": ball_owning_team,\n",
    "            \"ball_status\": ball_status,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # convert x, y and z to meters and set (0,0) at the bottom left.\n",
    "    all_tracking_data[\"x\"] = (\n",
    "        all_tracking_data[\"x\"] / 100 + game_metadata.loc[0, \"pitch_x\"] / 2\n",
    "    )\n",
    "    all_tracking_data[\"y\"] = (\n",
    "        all_tracking_data[\"y\"] / 100 + game_metadata.loc[0, \"pitch_y\"] / 2\n",
    "    )\n",
    "    all_tracking_data[\"z\"] = all_tracking_data[\"z\"] / 100\n",
    "\n",
    "    if remove_officials:\n",
    "        use = [\"1\", \"0\", \"10\"]\n",
    "        all_tracking_data = all_tracking_data[all_tracking_data.team.isin(use)]\n",
    "\n",
    "    if trim_dead_time:\n",
    "        if game_metadata.loc[0, \"period3_start\"] == 0:\n",
    "            all_tracking_data = all_tracking_data[\n",
    "                (\n",
    "                    (\n",
    "                        all_tracking_data[\"frameID\"]\n",
    "                        >= game_metadata.loc[0, \"period1_start\"]\n",
    "                    )\n",
    "                    & (\n",
    "                        all_tracking_data[\"frameID\"]\n",
    "                        <= game_metadata.loc[0, \"period1_end\"]\n",
    "                    )\n",
    "                )\n",
    "                | (\n",
    "                    (\n",
    "                        all_tracking_data[\"frameID\"]\n",
    "                        >= game_metadata.loc[0, \"period2_start\"]\n",
    "                    )\n",
    "                    & (\n",
    "                        all_tracking_data[\"frameID\"]\n",
    "                        <= game_metadata.loc[0, \"period2_end\"]\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "\n",
    "    all_tracking_data = all_tracking_data.reset_index(drop=True)\n",
    "\n",
    "    all_tracking_data.loc[all_tracking_data.loc[:, \"team\"] == \"0\", \"team\"] = home_away[\n",
    "        1\n",
    "    ]\n",
    "    all_tracking_data.loc[all_tracking_data.loc[:, \"team\"] == \"1\", \"team\"] = home_away[\n",
    "        0\n",
    "    ]\n",
    "\n",
    "    return all_tracking_data\n",
    "\n",
    "\n",
    "def parse_tracking_metadata(\n",
    "    metadata_filename: Union[str, pathlib.Path]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    An xml file will be parsed.\n",
    "\n",
    "    Output = Dataframe containing:\n",
    "        - FrameID of start and end of first and second half\n",
    "        - Length and width of the pitch\n",
    "\n",
    "    Remarks: period 3 and 4 have values of 0 if there was no overtime. Overtime can only happen in cupmatches.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(metadata_filename)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # period_startframe = []\n",
    "    # period_endframe = []\n",
    "\n",
    "    gamexml = root.findall(\"match\")[0]\n",
    "    # gamexml.findall('period').get('iStartFrame')\n",
    "\n",
    "    info_raw = []\n",
    "\n",
    "    for i in gamexml.iter(\"period\"):\n",
    "        # get the info from the ball node main chunk\n",
    "        #         print(int(i.get('iId')))\n",
    "        info_raw.append(i.get(\"iStartFrame\"))\n",
    "        info_raw.append(i.get(\"iEndFrame\"))\n",
    "\n",
    "    # # Create empty dict Capitals\n",
    "    game_metadata = pd.DataFrame()\n",
    "\n",
    "    # # Fill it with some values\n",
    "    game_metadata.loc[0, \"period1_start\"] = pd.to_numeric(info_raw[0])\n",
    "    game_metadata.loc[0, \"period1_end\"] = pd.to_numeric(info_raw[1])\n",
    "    game_metadata.loc[0, \"period2_start\"] = pd.to_numeric(info_raw[2])\n",
    "    game_metadata.loc[0, \"period2_end\"] = pd.to_numeric(info_raw[3])\n",
    "    game_metadata.loc[0, \"period3_start\"] = pd.to_numeric(info_raw[4])\n",
    "    game_metadata.loc[0, \"period3_end\"] = pd.to_numeric(info_raw[5])\n",
    "    game_metadata.loc[0, \"period4_start\"] = pd.to_numeric(info_raw[6])\n",
    "    game_metadata.loc[0, \"period4_end\"] = pd.to_numeric(info_raw[7])\n",
    "\n",
    "    for detail in root.iter(\"match\"):\n",
    "        game_metadata.loc[0, \"pitch_x\"] = pd.to_numeric(detail.get(\"fPitchXSizeMeters\"))\n",
    "        game_metadata.loc[0, \"pitch_y\"] = pd.to_numeric(detail.get(\"fPitchYSizeMeters\"))\n",
    "\n",
    "    return game_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from lxml import etree\n",
    "\n",
    "dir = os.getcwd()\n",
    "II=list()\n",
    "for file in glob.iglob(os.path.join( dir,'*/*.xml')):\n",
    "   with open(file) as f:\n",
    "      II.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aLLOFTHEM=II[::-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Player=create_playerDB(aLLOFTHEM[0])\n",
    "Case1=pd.DataFrame(Player[0])\n",
    "Case1['GameID']=aLLOFTHEM[0][-25:-16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "while i<len(aLLOFTHEM):\n",
    "    try:\n",
    "        Player=create_playerDB(aLLOFTHEM[i])\n",
    "        Case2=pd.DataFrame(Player[0])\n",
    "        Case2['GameID']=aLLOFTHEM[i][-25:-16]\n",
    "    except:\n",
    "        pass\n",
    "    Case1=pd.concat([Case1,Case2])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Case1['GameID'] = Case1['GameID'].replace(to_replace= r'-f', value= '', regex=True)\n",
    "Case1['GameID'] = Case1['GameID'].replace(to_replace= r'-', value= '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Case1['GameID']=Case1['GameID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Case1.to_csv(\"LisOFPlayers17-18.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eve=II[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "140\n",
      "138\n",
      "154\n",
      "164\n",
      "149\n",
      "152\n",
      "166\n",
      "139\n",
      "165\n",
      "154\n",
      "150\n",
      "147\n",
      "157\n",
      "146\n",
      "151\n",
      "137\n",
      "151\n",
      "147\n",
      "145\n",
      "150\n",
      "149\n",
      "146\n",
      "158\n",
      "145\n",
      "149\n",
      "146\n",
      "146\n",
      "155\n",
      "159\n",
      "154\n",
      "155\n",
      "145\n",
      "158\n",
      "159\n",
      "157\n",
      "164\n",
      "160\n",
      "137\n",
      "153\n",
      "155\n",
      "145\n",
      "153\n",
      "142\n",
      "155\n",
      "163\n",
      "148\n",
      "148\n",
      "142\n",
      "146\n",
      "143\n",
      "165\n",
      "149\n",
      "148\n",
      "149\n",
      "153\n",
      "142\n",
      "152\n",
      "148\n",
      "149\n",
      "144\n",
      "146\n",
      "157\n",
      "149\n",
      "139\n",
      "145\n",
      "145\n",
      "138\n",
      "150\n",
      "142\n",
      "149\n",
      "147\n",
      "141\n",
      "155\n",
      "148\n",
      "157\n",
      "146\n",
      "154\n",
      "157\n",
      "159\n",
      "136\n",
      "136\n",
      "141\n",
      "149\n",
      "147\n",
      "152\n",
      "153\n",
      "141\n",
      "150\n",
      "150\n",
      "149\n",
      "149\n",
      "144\n",
      "149\n",
      "157\n",
      "156\n",
      "147\n",
      "154\n",
      "142\n",
      "156\n",
      "157\n",
      "159\n",
      "159\n",
      "136\n",
      "160\n",
      "156\n",
      "145\n",
      "157\n",
      "142\n",
      "146\n",
      "150\n",
      "150\n",
      "150\n",
      "149\n",
      "155\n",
      "144\n",
      "154\n",
      "153\n",
      "153\n",
      "147\n",
      "132\n",
      "151\n",
      "144\n",
      "149\n",
      "142\n",
      "149\n",
      "154\n",
      "154\n",
      "155\n",
      "139\n",
      "159\n",
      "146\n",
      "149\n",
      "151\n",
      "147\n",
      "143\n",
      "155\n",
      "136\n",
      "140\n",
      "159\n",
      "147\n",
      "146\n",
      "150\n",
      "149\n",
      "143\n",
      "164\n",
      "156\n",
      "142\n",
      "139\n",
      "153\n",
      "165\n",
      "159\n",
      "148\n",
      "135\n",
      "153\n",
      "149\n",
      "144\n",
      "154\n",
      "152\n",
      "146\n",
      "157\n",
      "145\n",
      "152\n",
      "143\n",
      "156\n",
      "148\n",
      "151\n",
      "147\n",
      "157\n",
      "142\n",
      "141\n",
      "145\n",
      "159\n",
      "163\n",
      "158\n",
      "150\n",
      "154\n",
      "152\n",
      "150\n",
      "155\n",
      "159\n",
      "154\n",
      "147\n",
      "148\n",
      "154\n",
      "143\n",
      "148\n",
      "155\n",
      "150\n",
      "159\n",
      "153\n",
      "147\n",
      "134\n",
      "145\n",
      "153\n",
      "148\n",
      "159\n",
      "145\n",
      "138\n",
      "142\n",
      "153\n",
      "154\n",
      "147\n",
      "152\n",
      "145\n",
      "153\n",
      "159\n",
      "155\n",
      "143\n",
      "150\n",
      "155\n",
      "157\n",
      "136\n",
      "160\n",
      "146\n",
      "147\n",
      "147\n",
      "141\n",
      "133\n",
      "155\n",
      "150\n",
      "164\n",
      "155\n",
      "163\n",
      "158\n",
      "159\n",
      "149\n",
      "159\n",
      "148\n",
      "145\n",
      "148\n",
      "146\n",
      "136\n",
      "150\n",
      "145\n",
      "152\n",
      "151\n",
      "143\n",
      "149\n",
      "154\n",
      "155\n",
      "151\n",
      "149\n",
      "150\n",
      "151\n",
      "148\n",
      "145\n",
      "133\n",
      "147\n",
      "158\n",
      "153\n",
      "155\n",
      "154\n",
      "156\n",
      "152\n",
      "148\n",
      "163\n",
      "150\n",
      "141\n",
      "143\n",
      "145\n",
      "159\n",
      "151\n",
      "150\n",
      "145\n",
      "142\n",
      "149\n",
      "142\n",
      "131\n",
      "138\n",
      "165\n",
      "144\n",
      "139\n",
      "148\n",
      "144\n",
      "143\n",
      "149\n",
      "148\n",
      "152\n",
      "134\n",
      "155\n",
      "140\n",
      "152\n",
      "146\n",
      "138\n",
      "156\n",
      "136\n",
      "151\n",
      "158\n",
      "150\n",
      "145\n",
      "135\n",
      "146\n",
      "143\n",
      "150\n",
      "156\n",
      "150\n",
      "155\n",
      "140\n",
      "144\n",
      "141\n",
      "148\n",
      "144\n",
      "151\n",
      "153\n",
      "149\n",
      "134\n",
      "135\n",
      "143\n",
      "155\n",
      "158\n",
      "158\n"
     ]
    }
   ],
   "source": [
    "li=[]\n",
    "for filename in eve:\n",
    "    try:\n",
    "        Player=parse_f24(filename)\n",
    "        Case3=pd.DataFrame(Player[0])\n",
    "        li.append(Case3)\n",
    "        print(len(Case3.columns))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here=pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "535557"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def Filter(string, substr): \n",
    "    return [str for str in string if\n",
    "all(sub in str for sub in substr)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAN = [(c, Here[c].isna().mean()*100) for c in Here]\n",
    "NAN = pd.DataFrame(NAN, columns=[\"column_name\", \"percentage\"])\n",
    "NAN = NAN[NAN.percentage > 60]\n",
    "NAN.sort_values(\"percentage\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Na=NAN['column_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = Here.drop(list(Na),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here.to_csv(\"EventsUpdated1718.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
